import requests
from bs4 import BeautifulSoup
import os
import re
import time

# -----------------------------------------------------------
# [í…ŒìŠ¤íŠ¸ ì„¤ì •]
# ì‹¤ì‚¬ìš© ì‹œì—ëŠ” None ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
# -----------------------------------------------------------
TEST_LAST_ID = None
# TEST_LAST_ID = 1336480 

# -----------------------------------------------------------
# [ì„¤ì •] URL
# -----------------------------------------------------------
LIST_URL = "https://www.knu.ac.kr/wbbs/wbbs/bbs/btin/list.action?bbs_cde=1&menu_idx=67"
VIEW_URL_BASE = "https://www.knu.ac.kr/wbbs/wbbs/bbs/btin/viewBtin.action?btin.bbs_cde=1&btin.appl_no=000000&menu_idx=67&btin.doc_no="
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# -----------------------------------------------------------
# [ê³µí†µ í—¤ë”] í¬ë¡¬ ë¸Œë¼ìš°ì €ì¸ ì²™í•˜ê¸°
# -----------------------------------------------------------
COMMON_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'Referer': LIST_URL, 
    'Upgrade-Insecure-Requests': '1'
}

def get_post_content(url):
    try:
        requests.packages.urllib3.disable_warnings()
        
        response = requests.get(url, headers=COMMON_HEADERS, verify=False)
        response.encoding = 'UTF-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. ë‚´ìš©ì„ ì°¾ì„ í´ë˜ìŠ¤ í›„ë³´êµ° (board_cont ìµœìš°ì„ )
        candidates = [
            '.board_cont',      # â˜… ì‚¬ìš©ì í™•ì¸ ì™„ë£Œ
            '.board_view_con',  
            '.view_con',        
            '.bbs_view',        
            '.content',         
        ]

        content_div = None
        for selector in candidates:
            content_div = soup.select_one(selector)
            if content_div:
                break
        
        if content_div:
            return content_div.get_text(separator="\n", strip=True)
        else:
            return "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨: {e}"

def send_discord_message(webhook_url, title, link, doc_id, content):
    if not content: content = "(ë‚´ìš© ì—†ìŒ)"
    
    # ë””ìŠ¤ì½”ë“œ ê¸€ììˆ˜ ì œí•œ ê³ ë ¤ (1000ì)
    if len(content) > 1000:
        display_content = content[:1000] + "\n\n...(ë‚´ìš©ì´ ê¸¸ì–´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤. ë§í¬ë¥¼ í™•ì¸í•˜ì„¸ìš”)..."
    else:
        display_content = content

    data = {
        "content": "ğŸ”” **ê²½ë¶ëŒ€ í•™ì‚¬ê³µì§€ ì—…ë°ì´íŠ¸**",
        "embeds": [{
            "title": title,
            "description": display_content,
            "url": link,
            "color": 12916017, # KNU Red
            "footer": {"text": f"Doc ID: {doc_id}"}
        }]
    }
    
    try:
        requests.post(webhook_url, json=data)
        print(f"ğŸš€ [ì „ì†¡ ì„±ê³µ] {title}")
    except Exception as e:
        print(f"âŒ [ì „ì†¡ ì‹¤íŒ¨] {e}")

def main():
    requests.packages.urllib3.disable_warnings()
    print("--- [í¬ë¡¤ëŸ¬ ì‹œì‘] ---")

    # ID ì„¤ì •
    if TEST_LAST_ID is not None:
        last_id = int(TEST_LAST_ID)
        print(f"ğŸ¯ ê¸°ì¤€ ID (í…ŒìŠ¤íŠ¸): {last_id}")
    else:
        latest_id_path = os.path.join(BASE_DIR, 'latest_id.txt')
        try:
            with open(latest_id_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                last_id = int(content) if content else 0
        except FileNotFoundError:
            last_id = 0
        print(f"ğŸ“‚ ê¸°ì¤€ ID (íŒŒì¼): {last_id}")

    # ëª©ë¡ ì ‘ì†
    try:
        response = requests.get(LIST_URL, headers=COMMON_HEADERS, verify=False)
        response.encoding = 'UTF-8'
        soup = BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"ğŸš¨ ëª©ë¡ ì ‘ì† ì‹¤íŒ¨: {e}")
        return

    rows = soup.select("tbody > tr")
    if not rows: rows = soup.select("tr") 

    new_posts = []

    for row in rows:
        cols = row.select("td")
        if len(cols) < 2: continue
        
        title_tag = cols[1].find("a")
        if not title_tag: continue

        title = title_tag.text.strip()
        href_content = title_tag.get('href', '')
        
        match = re.search(r"(\d+)", href_content)
        if match:
            doc_id = int(match.group(1))
            
            if doc_id > last_id:
                real_link = VIEW_URL_BASE + str(doc_id)
                new_posts.append({'id': doc_id, 'title': title, 'link': real_link})

    if new_posts:
        print(f"âœ¨ ì´ {len(new_posts)}ê°œì˜ ìƒˆ ê³µì§€ ë°œê²¬!")
        
        # ê³¼ê±°ìˆœ ì •ë ¬ (ì˜›ë‚  ê¸€ -> ìµœì‹  ê¸€)
        new_posts.sort(key=lambda x: x['id'])
        
        webhook_url = os.environ.get("DISCORD_WEBHOOK_URL") or os.environ.get("DISCORD_WEBHOOK")
        
        if webhook_url:
            for post in new_posts:
                content = get_post_content(post['link'])
                send_discord_message(webhook_url, post['title'], post['link'], post['id'], content)
                time.sleep(1)
        else:
            print("âŒ WebHook URL ì—†ìŒ")

        # íŒŒì¼ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ì•„ë‹ ë•Œë§Œ)
        if TEST_LAST_ID is None:
            max_id = max(p['id'] for p in new_posts)
            latest_id_path = os.path.join(BASE_DIR, 'latest_id.txt')
            with open(latest_id_path, 'w', encoding='utf-8') as f:
                f.write(str(max_id))
            print(f"ğŸ’¾ ID ì—…ë°ì´íŠ¸ ì™„ë£Œ: {max_id}")
    else:
        print("ğŸ’¤ ìƒˆ ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
