import requests
from bs4 import BeautifulSoup
import os
import re
import time

# -----------------------------------------------------------
# [í…ŒìŠ¤íŠ¸ ì„¤ì •]
# -----------------------------------------------------------
TEST_LAST_ID = 1336480 

# -----------------------------------------------------------
# [ì„¤ì •] URL
# -----------------------------------------------------------
LIST_URL = "https://www.knu.ac.kr/wbbs/wbbs/bbs/btin/list.action?bbs_cde=1&menu_idx=67"
VIEW_URL_BASE = "https://www.knu.ac.kr/wbbs/wbbs/bbs/btin/viewBtin.action?btin.bbs_cde=1&btin.appl_no=000000&menu_idx=67&btin.doc_no="
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

def get_post_content(url):
    try:
        requests.packages.urllib3.disable_warnings()
        # í—¤ë”ë¥¼ í•¨ìˆ˜ ì•ˆì—ì„œë„ ë™ì¼í•˜ê²Œ ì‚¬ìš©
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive'
        }
        response = requests.get(url, headers=headers, verify=False)
        response.encoding = 'UTF-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        content_div = soup.select_one('.board_view_con') or soup.select_one('.view_con')
        if content_div:
            return content_div.get_text(separator="\n", strip=True)
        return "ë³¸ë¬¸ ì—†ìŒ"
    except:
        return "í¬ë¡¤ë§ ì‹¤íŒ¨"

def send_discord_message(webhook_url, title, link, doc_id, content):
    if len(content) > 1500:
        display_content = content[:1500] + "\n\n...(ë‚´ìš© ìƒëµ)..."
    else:
        display_content = content

    data = {
        "content": "ğŸ”” **ê²½ë¶ëŒ€ í•™ì‚¬ê³µì§€ ì—…ë°ì´íŠ¸**",
        "embeds": [{
            "title": title,
            "description": display_content,
            "url": link,
            "color": 12916017,
            "footer": {"text": f"Doc ID: {doc_id}"}
        }]
    }
    try:
        requests.post(webhook_url, json=data)
        print(f"ğŸš€ [ì „ì†¡ ì„±ê³µ] {title}")
    except Exception as e:
        print(f"âŒ [ì „ì†¡ ì‹¤íŒ¨] {e}")

def main():
    requests.packages.urllib3.disable_warnings()
    
    print("--- [í¬ë¡¤ëŸ¬ ì‹œì‘ (ë³´ì•ˆ ìš°íšŒ ì‹œë„)] ---")

    if TEST_LAST_ID is not None:
        last_id = int(TEST_LAST_ID)
        print(f"ğŸ¯ ê¸°ì¤€ ID (í…ŒìŠ¤íŠ¸): {last_id}")
    else:
        latest_id_path = os.path.join(BASE_DIR, 'latest_id.txt')
        try:
            with open(latest_id_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                last_id = int(content) if content else 0
        except FileNotFoundError:
            last_id = 0
        print(f"ğŸ“‚ ê¸°ì¤€ ID (íŒŒì¼): {last_id}")

    # 1. í—¤ë” ê°•í™” (ì§„ì§œ í¬ë¡¬ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê¸°)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Referer': 'https://www.knu.ac.kr/',
        'Upgrade-Insecure-Requests': '1'
    }

    try:
        # ì„¸ì…˜ ì‚¬ìš© (ì¿ í‚¤ ìœ ì§€ ë“±ì„ ìœ„í•´)
        session = requests.Session()
        response = session.get(LIST_URL, headers=headers, verify=False)
        response.encoding = 'UTF-8'
        
        print(f"ğŸ“¡ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
        
        soup = BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"ğŸš¨ ì ‘ì† ì‹¤íŒ¨: {e}")
        return

    # 2. ì„ íƒì(Selector) ìœ ì—°í™”
    # tbody > tr ì´ ì•ˆ ë¨¹í ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê·¸ëƒ¥ trì„ ë‹¤ ì°¾ê³  í•„í„°ë§
    rows = soup.select("tbody > tr")
    if not rows:
        print("âš ï¸ 'tbody > tr'ë¡œ í–‰ì„ ëª» ì°¾ìŒ. 'tr' ì „ì²´ ê²€ìƒ‰ ì‹œë„...")
        rows = soup.select("tr")

    print(f"ğŸ” ì´ {len(rows)}ê°œì˜ í–‰ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.")

    # [ë””ë²„ê¹…] ë§Œì•½ 0ê°œë¼ë©´ HTML ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸)
    if len(rows) == 0:
        print("\nğŸš¨ [ì‹¬ê°] ê²Œì‹œê¸€ì„ í•˜ë‚˜ë„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê°€ì ¸ì˜¨ HTML ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")
        print("----------------------------------------------------------------")
        # HTMLì˜ ì œëª©ê³¼ ì•ë¶€ë¶„ 500ìë§Œ ì¶œë ¥
        print(f"Title: {soup.title.text if soup.title else 'No Title'}")
        print(soup.prettify()[:1000]) 
        print("----------------------------------------------------------------")
        return

    new_posts = []

    for i, row in enumerate(rows):
        cols = row.select("td")
        # ë°ì´í„°ê°€ ì—†ëŠ” í–‰(í—¤ë” ë“±)ì€ ê±´ë„ˆëœ€
        if len(cols) < 2: continue
        
        # í™”ë©´ ë²ˆí˜¸ í™•ì¸
        visible_num = cols[0].text.strip()
        
        # ì œëª© íƒœê·¸ ì°¾ê¸°
        title_tag = cols[1].find("a")
        if not title_tag: continue # ì œëª© ë§í¬ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€

        title = title_tag.text.strip()
        href_content = title_tag.get('href', '')
        
        # URL ê³ ìœ ë²ˆí˜¸ ì¶”ì¶œ
        match = re.search(r"(\d+)", href_content)
        
        if match:
            doc_id = int(match.group(1))
            
            # ë´‡ ë¡œê·¸: í˜„ì¬ ë³´ê³  ìˆëŠ” ê¸€ ì¶œë ¥
            print(f"[{i}] ë²ˆí˜¸:{doc_id} | ì œëª©:{title[:10]}...", end=" ")

            if doc_id > last_id:
                print("âœ… [ìƒˆ ê¸€]")
                real_link = VIEW_URL_BASE + str(doc_id)
                new_posts.append({
                    'id': doc_id,
                    'title': title,
                    'link': real_link
                })
            else:
                print("â¹ï¸ [ì˜›ë‚  ê¸€]")
        else:
            # ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‹¨ìˆœ ë§í¬ì´ê±°ë‚˜ ê³µì§€)
            pass

    print(f"\nâœ¨ ë°œê²¬ëœ ìƒˆ ê³µì§€: {len(new_posts)}ê°œ")

    if new_posts:
        new_posts.sort(key=lambda x: x['id'])
        
        webhook_url = os.environ.get("DISCORD_WEBHOOK_URL") or os.environ.get("DISCORD_WEBHOOK")
        
        if webhook_url:
            for post in new_posts:
                content = get_post_content(post['link'])
                send_discord_message(webhook_url, post['title'], post['link'], post['id'], content)
                time.sleep(1)
        else:
            print("âŒ WebHook URL ì—†ìŒ")

        if TEST_LAST_ID is None:
            max_id = max(p['id'] for p in new_posts)
            latest_id_path = os.path.join(BASE_DIR, 'latest_id.txt')
            with open(latest_id_path, 'w', encoding='utf-8') as f:
                f.write(str(max_id))
            print(f"ğŸ’¾ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {max_id}")
    else:
        print("ğŸ’¤ ì „ì†¡í•  ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
